services:
  ollama:
    volumes:
      - ~/ollama/ollama:/root/.ollama
    container_name: ollama
    #pull_policy: always
    tty: true
    #restart: unless-stopped
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    networks:
      - app-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
  faster-whisper-server-cuda:
    image: fedirz/faster-whisper-server:latest-cuda
    restart: unless-stopped
    ports:
      - 11800:11800
    volumes:
      - hugging_face_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    environment:
      - WHISPER__MODEL=Systran/faster-distil-whisper-small.en
      - UVICORN_HOST=0.0.0.0
      - UVICORN_PORT=11800
  tts:
    image: ghcr.io/coqui-ai/tts:latest
    entrypoint: python3
    command: TTS/server/server.py --model_name tts_models/en/ljspeech/tacotron2-DDC --use_cuda true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    volumes:
      - ./volumes/tts:/root/.local/share/tts
    ports:
      - "5002:5002"
networks:
  app-network:
    driver: bridge
volumes:
  hugging_face_cache:
    driver: local
    driver_opts:
      type: none
      device: ~/.cache/huggingface
      o: bind
